"""
sabangnet_API RAG ì‹œìŠ¤í…œ
Faissë¥¼ ì´ìš©í•œ ë²¡í„° ê²€ìƒ‰ ê¸°ë°˜ ì§ˆë¬¸-ë‹µë³€ ì‹œìŠ¤í…œ
"""

import os
import json
import sqlite3
import pickle
import hashlib
import re
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import asyncio
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install openai")

# ë²¡í„° ê²€ìƒ‰ ë¹„í™œì„±í™” - GPT ê¸°ë°˜ ê²€ìƒ‰ ì‚¬ìš©
    VECTOR_AVAILABLE = False

@dataclass
class Document:
    """ë¬¸ì„œ êµ¬ì¡°ì²´"""
    id: str
    title: str
    content: str
    category: str
    tags: List[str]
    file_path: str
    line_start: int
    line_end: int
    created_at: datetime
    metadata: Dict[str, Any]

@dataclass
class QueryResult:
    """ê²€ìƒ‰ ê²°ê³¼ êµ¬ì¡°ì²´"""
    document: Document
    score: float
    matched_sections: List[str]

class SabangnetRAGSystem:
    """ì‚¬ë°©ë„· RAG ì‹œìŠ¤í…œ - Faiss ê¸°ë°˜ + GPT"""
    
    def __init__(self, sabangnet_path: str = "../../../sabangnet_API", db_path: str = "sabangnet_rag.db"):
        self.sabangnet_path = Path(sabangnet_path)
        self.db_path = db_path
        self.documents: List[Document] = []
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.openai_client = None
        if OPENAI_AVAILABLE:
            api_key = os.getenv("GPTKEY")
            if api_key:
                self.openai_client = OpenAI(api_key=api_key)
                print("GPT API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                print("GPTKEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        print("GPT ê¸°ë°˜ ê²€ìƒ‰ ëª¨ë“œë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        self._init_database()
        self._load_documents()
    
    def _init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ë¬¸ì„œ í…Œì´ë¸” ìƒì„±
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS documents (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                content TEXT NOT NULL,
                category TEXT NOT NULL,
                tags TEXT,
                file_path TEXT,
                line_start INTEGER,
                line_end INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                metadata TEXT
            )
        ''')
        
        # ë²¡í„° ì„ë² ë”© í…Œì´ë¸” ì œê±° - GPT ê¸°ë°˜ ê²€ìƒ‰ ì‚¬ìš©
        
        conn.commit()
        conn.close()
    
    def _load_documents(self):
        """ì €ì¥ëœ ë¬¸ì„œë“¤ì„ ë©”ëª¨ë¦¬ë¡œ ë¡œë“œ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT * FROM documents')
        rows = cursor.fetchall()
        
        for row in rows:
            doc = Document(
                id=row[0],
                title=row[1],
                content=row[2],
                category=row[3],
                tags=json.loads(row[4]) if row[4] else [],
                file_path=row[5],
                line_start=row[6],
                line_end=row[7],
                created_at=datetime.fromisoformat(row[8]),
                metadata=json.loads(row[9]) if row[9] else {}
            )
            self.documents.append(doc)
        
        conn.close()
        print(f"ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(self.documents)}")
    
    # Faiss ì¸ë±ìŠ¤ êµ¬ì¶• ì œê±° - GPT ê¸°ë°˜ ê²€ìƒ‰ ì‚¬ìš©
    
    def extract_sabangnet_documents(self):
        """sabangnet_APIì—ì„œ ëª¨ë“  ë¬¸ì„œ ì¶”ì¶œ"""
        if not self.sabangnet_path.exists():
            print(f"sabangnet_API ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.sabangnet_path}")
            return
        
        print("sabangnet_API ë¬¸ì„œ ì¶”ì¶œ ì‹œì‘...")
        
        # ì¶”ì¶œí•  íŒŒì¼ í™•ì¥ì
        target_extensions = {'.py', '.md', '.txt', '.yml', '.yaml', '.json', '.ini'}
        
        # ì œì™¸í•  ë””ë ‰í† ë¦¬
        exclude_dirs = {'__pycache__', '.git', 'node_modules', '.pytest_cache', 'venv', 'env'}
        
        total_files = 0
        for root, dirs, files in os.walk(self.sabangnet_path):
            # ì œì™¸ ë””ë ‰í† ë¦¬ í•„í„°ë§
            dirs[:] = [d for d in dirs if d not in exclude_dirs]
            
            for file in files:
                if Path(file).suffix in target_extensions:
                    file_path = Path(root) / file
                    self._extract_file_content(file_path)
                    total_files += 1
        
        print(f"ì´ {total_files}ê°œ íŒŒì¼ì—ì„œ ë¬¸ì„œ ì¶”ì¶œ ì™„ë£Œ")
        print(f"ì´ {len(self.documents)}ê°œ ë¬¸ì„œ ìƒì„±")
    
    def _extract_file_content(self, file_path: Path):
        """ê°œë³„ íŒŒì¼ì—ì„œ ë‚´ìš© ì¶”ì¶œ - ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„ë¡œ ë¶„í• """
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            if not content.strip():
                return
            
            # íŒŒì¼ íƒ€ì…ì— ë”°ë¥¸ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            category = self._get_file_category(file_path)
            
            # íŒŒì¼ ë‚´ìš© ë¶„ì„
            content_analysis = self._analyze_file_content(content, file_path)
            
            # íŒŒì¼ íƒ€ì…ë³„ë¡œ ë‹¤ë¥¸ íŒŒì‹± ì „ëµ ì ìš©
            if file_path.suffix == '.py':
                # Python íŒŒì¼: í•¨ìˆ˜, í´ë˜ìŠ¤, API ì—”ë“œí¬ì¸íŠ¸ ë‹¨ìœ„ë¡œ ë¶„í• 
                chunks = self._parse_python_file(content, file_path)
            elif file_path.suffix in ['.md', '.txt']:
                # ë¬¸ì„œ íŒŒì¼: ì„¹ì…˜ ë‹¨ìœ„ë¡œ ë¶„í• 
                chunks = self._parse_document_file(content, file_path)
            elif file_path.suffix in ['.yml', '.yaml', '.json']:
                # ì„¤ì • íŒŒì¼: ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ
                chunks = [{'type': 'config', 'content': content, 'title': f"ì„¤ì •: {file_path.name}"}]
            else:
                # ê¸°íƒ€ íŒŒì¼: ê¸°ë³¸ ì²­í¬ ë¶„í• 
                chunks = self._split_content_into_chunks(content, file_path)
            
            for i, chunk_data in enumerate(chunks):
                if isinstance(chunk_data, dict):
                    # êµ¬ì¡°í™”ëœ ì²­í¬ ë°ì´í„°
                    doc_id = hashlib.md5(f"{file_path}_{chunk_data.get('type', 'chunk')}_{i}_{datetime.now()}".encode()).hexdigest()
                    
                    # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ìƒì„±
                    embedding_text = self._create_contextual_embedding_text(
                        chunk_data.get('content', ''),
                        file_path,
                        category,
                        chunk_data,
                        content_analysis
                    )
                    
                    doc = Document(
                        id=doc_id,
                        title=chunk_data.get('title', f"{file_path.name} (ì²­í¬ {i+1})"),
                        content=embedding_text,  # ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ì‚¬ìš©
                        category=category,
                        tags=chunk_data.get('tags', []) + self._extract_tags(chunk_data.get('content', ''), file_path),
                        file_path=str(file_path),
                        line_start=chunk_data.get('line_start', 0),
                        line_end=chunk_data.get('line_end', 0),
                        created_at=datetime.now(),
                        metadata={
                            "file_type": file_path.suffix,
                            "file_size": len(content),
                            "chunk_index": i,
                            "total_chunks": len(chunks),
                            "chunk_type": chunk_data.get('type', 'general'),
                            "function_name": chunk_data.get('function_name', ''),
                            "class_name": chunk_data.get('class_name', ''),
                            "api_endpoint": chunk_data.get('api_endpoint', ''),
                            "table_name": chunk_data.get('table_name', ''),
                            "business_domain": content_analysis.get('business_domain', ''),
                            "technical_components": content_analysis.get('technical_components', []),
                            "data_entities": content_analysis.get('data_entities', []),
                            "original_content": chunk_data.get('content', '')  # ì›ë³¸ ë‚´ìš© ë³´ì¡´
                        }
                    )
                else:
                    # ì¼ë°˜ í…ìŠ¤íŠ¸ ì²­í¬
                    doc_id = hashlib.md5(f"{file_path}_{i}_{datetime.now()}".encode()).hexdigest()
                    
                    embedding_text = self._create_contextual_embedding_text(
                        chunk_data,
                        file_path,
                        category,
                        {'type': 'general', 'content': chunk_data},
                        content_analysis
                    )
                    
                    doc = Document(
                        id=doc_id,
                        title=f"{file_path.name} (ì²­í¬ {i+1})",
                        content=embedding_text,
                        category=category,
                        tags=self._extract_tags(chunk_data, file_path),
                        file_path=str(file_path),
                        line_start=0,
                        line_end=0,
                        created_at=datetime.now(),
                        metadata={
                            "file_type": file_path.suffix,
                            "file_size": len(content),
                            "chunk_index": i,
                            "total_chunks": len(chunks),
                            "business_domain": content_analysis.get('business_domain', ''),
                            "original_content": chunk_data
                        }
                    )
                
                self._save_document(doc)
                
        except Exception as e:
            print(f"íŒŒì¼ ì¶”ì¶œ ì‹¤íŒ¨ {file_path}: {e}")
    
    def _get_file_category(self, file_path: Path) -> str:
        """íŒŒì¼ ê²½ë¡œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ê²°ì •"""
        path_str = str(file_path)
        
        if 'models/' in path_str:
            return 'database_model'
        elif 'controller/' in path_str:
            return 'controller'
        elif 'services/' in path_str:
            return 'service'
        elif 'api/' in path_str:
            return 'api_endpoint'
        elif 'schemas/' in path_str:
            return 'schema'
        elif 'repository/' in path_str:
            return 'repository'
        elif 'utils/' in path_str:
            return 'utility'
        elif 'tests/' in path_str:
            return 'test'
        elif file_path.name in ['README.md', 'requirements.txt', 'app.py', 'main.py']:
            return 'main'
        else:
            return 'other'
    
    def _split_content_into_chunks(self, content: str, file_path: Path) -> List[str]:
        """ë‚´ìš©ì„ ì²­í¬ë¡œ ë¶„í• """
        max_chunk_size = 2000  # ìµœëŒ€ ì²­í¬ í¬ê¸°
        
        if len(content) <= max_chunk_size:
            return [content]
        
        chunks = []
        lines = content.split('\n')
        current_chunk = []
        current_size = 0
        
        for line in lines:
            line_size = len(line) + 1  # +1 for newline
            
            if current_size + line_size > max_chunk_size and current_chunk:
                chunks.append('\n'.join(current_chunk))
                current_chunk = [line]
                current_size = line_size
            else:
                current_chunk.append(line)
                current_size += line_size
        
        if current_chunk:
            chunks.append('\n'.join(current_chunk))
        
        return chunks
    
    def _analyze_file_content(self, content: str, file_path: Path) -> Dict:
        """íŒŒì¼ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì˜ë¯¸ìˆëŠ” ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        analysis = {
            'primary_category': 'unknown',
            'secondary_categories': [],
            'business_domain': 'unknown',
            'technical_components': [],
            'data_entities': [],
            'api_endpoints': [],
            'dependencies': []
        }
        
        # 1. ë¹„ì¦ˆë‹ˆìŠ¤ ë„ë©”ì¸ ë¶„ì„
        if any(keyword in content.lower() for keyword in ['order', 'ì£¼ë¬¸', 'order_list']):
            analysis['business_domain'] = 'order_management'
            analysis['secondary_categories'].append('ì£¼ë¬¸ê´€ë¦¬')
        
        if any(keyword in content.lower() for keyword in ['product', 'ìƒí’ˆ', 'product_create']):
            analysis['business_domain'] = 'product_management'
            analysis['secondary_categories'].append('ìƒí’ˆê´€ë¦¬')
        
        if any(keyword in content.lower() for keyword in ['batch', 'ë°°ì¹˜', 'macro']):
            analysis['business_domain'] = 'batch_processing'
            analysis['secondary_categories'].append('ë°°ì¹˜ì²˜ë¦¬')
        
        if any(keyword in content.lower() for keyword in ['shipment', 'ë°°ì†¡', 'hanjin']):
            analysis['business_domain'] = 'shipping_management'
            analysis['secondary_categories'].append('ë°°ì†¡ê´€ë¦¬')
        
        # 2. ê¸°ìˆ ì  ì»´í¬ë„ŒíŠ¸ ë¶„ì„
        if 'class ' in content and 'Base' in content:
            analysis['technical_components'].append('database_model')
        
        if '@router.' in content or 'APIRouter' in content:
            analysis['technical_components'].append('api_endpoint')
        
        if 'async def' in content and 'service' in str(file_path):
            analysis['technical_components'].append('service_layer')
        
        if 'controller' in str(file_path).lower():
            analysis['technical_components'].append('controller')
        
        # 3. ë°ì´í„° ì—”í‹°í‹° ì¶”ì¶œ
        table_names = re.findall(r'__tablename__ = ["\']([^"\']+)["\']', content)
        analysis['data_entities'].extend(table_names)
        
        # 4. API ì—”ë“œí¬ì¸íŠ¸ ì¶”ì¶œ
        endpoints = re.findall(r'@router\.(get|post|put|delete)\(["\']([^"\']+)["\']', content)
        analysis['api_endpoints'].extend([f"{method.upper()} {path}" for method, path in endpoints])
        
        return analysis
    
    def _parse_python_file(self, content: str, file_path: Path) -> List[Dict]:
        """Python íŒŒì¼ì„ ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„ë¡œ ë¶„í• """
        chunks = []
        lines = content.split('\n')
        
        # 1. í´ë˜ìŠ¤ ì¶”ì¶œ
        class_pattern = r'^class\s+(\w+).*?:'
        for i, line in enumerate(lines):
            match = re.match(class_pattern, line.strip())
            if match:
                class_name = match.group(1)
                class_content, end_line = self._extract_class_content(lines, i)
                
                chunks.append({
                    'type': 'class',
                    'title': f"í´ë˜ìŠ¤: {class_name}",
                    'content': class_content,
                    'class_name': class_name,
                    'line_start': i + 1,
                    'line_end': end_line,
                    'tags': ['í´ë˜ìŠ¤', class_name]
                })
        
        # 2. í•¨ìˆ˜ ì¶”ì¶œ
        function_pattern = r'^(async\s+)?def\s+(\w+).*?:'
        for i, line in enumerate(lines):
            match = re.match(function_pattern, line.strip())
            if match:
                function_name = match.group(2)
                func_content, end_line = self._extract_function_content(lines, i)
                
                chunks.append({
                    'type': 'function',
                    'title': f"í•¨ìˆ˜: {function_name}",
                    'content': func_content,
                    'function_name': function_name,
                    'line_start': i + 1,
                    'line_end': end_line,
                    'tags': ['í•¨ìˆ˜', function_name]
                })
        
        # 3. API ì—”ë“œí¬ì¸íŠ¸ ì¶”ì¶œ
        api_pattern = r'@router\.(get|post|put|delete)\(["\']([^"\']+)["\']'
        for i, line in enumerate(lines):
            match = re.search(api_pattern, line)
            if match:
                method = match.group(1).upper()
                path = match.group(2)
                
                # ë‹¤ìŒ í•¨ìˆ˜ê¹Œì§€ì˜ ë‚´ìš© ì¶”ì¶œ
                func_content, end_line = self._extract_function_content(lines, i + 1)
                
                chunks.append({
                    'type': 'api_endpoint',
                    'title': f"API: {method} {path}",
                    'content': func_content,
                    'api_endpoint': f"{method} {path}",
                    'line_start': i + 1,
                    'line_end': end_line,
                    'tags': ['API', method, path]
                })
        
        # 4. í…Œì´ë¸” ëª¨ë¸ ì¶”ì¶œ
        table_pattern = r'__tablename__ = ["\']([^"\']+)["\']'
        for i, line in enumerate(lines):
            match = re.search(table_pattern, line)
            if match:
                table_name = match.group(1)
                
                # í´ë˜ìŠ¤ ì „ì²´ ë‚´ìš© ì¶”ì¶œ
                class_start = self._find_class_start(lines, i)
                if class_start >= 0:
                    class_content, end_line = self._extract_class_content(lines, class_start)
                    
                    chunks.append({
                        'type': 'database_model',
                        'title': f"í…Œì´ë¸”: {table_name}",
                        'content': class_content,
                        'table_name': table_name,
                        'line_start': class_start + 1,
                        'line_end': end_line,
                        'tags': ['í…Œì´ë¸”', table_name]
                    })
        
        return chunks if chunks else [{'type': 'general', 'content': content, 'title': f"íŒŒì¼: {file_path.name}"}]
    
    def _parse_document_file(self, content: str, file_path: Path) -> List[Dict]:
        """ë¬¸ì„œ íŒŒì¼ì„ ì„¹ì…˜ ë‹¨ìœ„ë¡œ ë¶„í• """
        chunks = []
        sections = content.split('\n\n')
        
        for i, section in enumerate(sections):
            if section.strip():
                # ì„¹ì…˜ ì œëª© ì¶”ì¶œ
                lines = section.strip().split('\n')
                title = lines[0] if lines else f"ì„¹ì…˜ {i+1}"
                
                chunks.append({
                    'type': 'document_section',
                    'title': title,
                    'content': section.strip(),
                    'tags': ['ë¬¸ì„œ', 'ì„¹ì…˜']
                })
        
        return chunks if chunks else [{'type': 'document', 'content': content, 'title': f"ë¬¸ì„œ: {file_path.name}"}]
    
    def _extract_class_content(self, lines: List[str], start_line: int) -> Tuple[str, int]:
        """í´ë˜ìŠ¤ ë‚´ìš© ì¶”ì¶œ"""
        content_lines = []
        indent_level = len(lines[start_line]) - len(lines[start_line].lstrip())
        current_line = start_line
        
        # í´ë˜ìŠ¤ ì‹œì‘ ë¼ì¸ ì¶”ê°€
        content_lines.append(lines[start_line])
        current_line += 1
        
        # í´ë˜ìŠ¤ ë‚´ìš© ì¶”ì¶œ
        while current_line < len(lines):
            line = lines[current_line]
            
            # ë¹ˆ ë¼ì¸ì€ ì¶”ê°€
            if not line.strip():
                content_lines.append(line)
                current_line += 1
                continue
            
            # í˜„ì¬ ë“¤ì—¬ì“°ê¸° ë ˆë²¨ í™•ì¸
            current_indent = len(line) - len(line.lstrip())
            
            # ê°™ì€ ë ˆë²¨ì˜ í´ë˜ìŠ¤ë‚˜ í•¨ìˆ˜ê°€ ì‹œì‘ë˜ë©´ ì¤‘ë‹¨
            if current_indent <= indent_level and (line.strip().startswith('class ') or line.strip().startswith('def ')):
                break
            
            content_lines.append(line)
            current_line += 1
        
        return '\n'.join(content_lines), current_line - 1
    
    def _extract_function_content(self, lines: List[str], start_line: int) -> Tuple[str, int]:
        """í•¨ìˆ˜ ë‚´ìš© ì¶”ì¶œ"""
        content_lines = []
        indent_level = len(lines[start_line]) - len(lines[start_line].lstrip())
        current_line = start_line
        
        # í•¨ìˆ˜ ì‹œì‘ ë¼ì¸ ì¶”ê°€
        content_lines.append(lines[start_line])
        current_line += 1
        
        # í•¨ìˆ˜ ë‚´ìš© ì¶”ì¶œ
        while current_line < len(lines):
            line = lines[current_line]
            
            # ë¹ˆ ë¼ì¸ì€ ì¶”ê°€
            if not line.strip():
                content_lines.append(line)
                current_line += 1
                continue
            
            # í˜„ì¬ ë“¤ì—¬ì“°ê¸° ë ˆë²¨ í™•ì¸
            current_indent = len(line) - len(line.lstrip())
            
            # ê°™ì€ ë ˆë²¨ì˜ í•¨ìˆ˜ë‚˜ í´ë˜ìŠ¤ê°€ ì‹œì‘ë˜ë©´ ì¤‘ë‹¨
            if current_indent <= indent_level and (line.strip().startswith('def ') or line.strip().startswith('class ')):
                break
            
            content_lines.append(line)
            current_line += 1
        
        return '\n'.join(content_lines), current_line - 1
    
    def _find_class_start(self, lines: List[str], line_index: int) -> int:
        """í…Œì´ë¸” ì •ì˜ê°€ í¬í•¨ëœ í´ë˜ìŠ¤ì˜ ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°"""
        for i in range(line_index, -1, -1):
            if lines[i].strip().startswith('class '):
                return i
        return -1
    
    def _create_contextual_embedding_text(self, content: str, file_path: Path, category: str, chunk_data: Dict, content_analysis: Dict) -> str:
        """ë¬¸ì„œì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•œ ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ìƒì„±"""
        context_parts = []
        
        # 1. ê¸°ë³¸ ì •ë³´
        context_parts.append(f"íŒŒì¼: {file_path.name}")
        context_parts.append(f"ê²½ë¡œ: {file_path}")
        context_parts.append(f"ì¹´í…Œê³ ë¦¬: {category}")
        
        # 2. ì²­í¬ íƒ€ì… ì •ë³´
        chunk_type = chunk_data.get('type', 'general')
        context_parts.append(f"íƒ€ì…: {chunk_type}")
        
        # 3. ë©”íƒ€ë°ì´í„° ì •ë³´
        if chunk_data.get('class_name'):
            context_parts.append(f"í´ë˜ìŠ¤: {chunk_data['class_name']}")
        
        if chunk_data.get('function_name'):
            context_parts.append(f"í•¨ìˆ˜: {chunk_data['function_name']}")
        
        if chunk_data.get('api_endpoint'):
            context_parts.append(f"API: {chunk_data['api_endpoint']}")
        
        if chunk_data.get('table_name'):
            context_parts.append(f"í…Œì´ë¸”: {chunk_data['table_name']}")
        
        # 4. ë¹„ì¦ˆë‹ˆìŠ¤ ë„ë©”ì¸ ì •ë³´
        if content_analysis.get('business_domain'):
            context_parts.append(f"ë„ë©”ì¸: {content_analysis['business_domain']}")
        
        # 5. ê¸°ìˆ ì  ì»´í¬ë„ŒíŠ¸
        if content_analysis.get('technical_components'):
            context_parts.append(f"ê¸°ìˆ ì»´í¬ë„ŒíŠ¸: {', '.join(content_analysis['technical_components'])}")
        
        # 6. ë°ì´í„° ì—”í‹°í‹°
        if content_analysis.get('data_entities'):
            context_parts.append(f"ë°ì´í„°ì—”í‹°í‹°: {', '.join(content_analysis['data_entities'])}")
        
        # 7. ì‹¤ì œ ì½”ë“œ ë‚´ìš©
        context_parts.append(f"ë‚´ìš©: {content}")
        
        # 8. íƒœê·¸ ì •ë³´
        if chunk_data.get('tags'):
            context_parts.append(f"íƒœê·¸: {', '.join(chunk_data['tags'])}")
        
        return "\n".join(context_parts)
    
    def _extract_tags(self, content: str, file_path: Path) -> List[str]:
        """ë‚´ìš©ì—ì„œ íƒœê·¸ ì¶”ì¶œ"""
        tags = []
        
        # íŒŒì¼ëª…ì—ì„œ íƒœê·¸ ì¶”ì¶œ
        file_name = file_path.stem.lower()
        if 'order' in file_name:
            tags.append('ì£¼ë¬¸')
        if 'product' in file_name:
            tags.append('ìƒí’ˆ')
        if 'user' in file_name:
            tags.append('ì‚¬ìš©ì')
        if 'auth' in file_name:
            tags.append('ì¸ì¦')
        if 'api' in file_name:
            tags.append('API')
        if 'test' in file_name:
            tags.append('í…ŒìŠ¤íŠ¸')
        
        # ë‚´ìš©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = ['smile', 'hanjin', 'ecount', 'gmarket', 'batch', 'macro', 'erp']
        for keyword in keywords:
            if keyword.lower() in content.lower():
                tags.append(keyword)
        
        return list(set(tags))  # ì¤‘ë³µ ì œê±°
    
    def _save_document(self, doc: Document):
        """ë¬¸ì„œë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO documents 
            (id, title, content, category, tags, file_path, line_start, line_end, created_at, metadata)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            doc.id, doc.title, doc.content, doc.category,
            json.dumps(doc.tags), doc.file_path, doc.line_start, doc.line_end,
            doc.created_at.isoformat(), json.dumps(doc.metadata)
        ))
        
        # ë²¡í„° ì„ë² ë”© ìƒì„± ì œê±° - GPT ê¸°ë°˜ ê²€ìƒ‰ ì‚¬ìš©
        
        conn.commit()
        conn.close()
        
        self.documents.append(doc)
    
    def search(self, query: str, limit: int = 10, category: str = None) -> List[QueryResult]:
        """GPT ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰"""
        return self._gpt_based_search(query, limit, category)
    
    def _gpt_based_search(self, query: str, limit: int, category: str = None) -> List[QueryResult]:
        """GPT ê¸°ë°˜ ì§€ëŠ¥í˜• ë¬¸ì„œ ê²€ìƒ‰ - í† í° ìµœì í™”"""
        if not self.openai_client:
            return self._fallback_search(query, limit, category)
        
        try:
            # 1ë‹¨ê³„: í‚¤ì›Œë“œ ê¸°ë°˜ ì‚¬ì „ í•„í„°ë§
            filtered_docs = self._pre_filter_documents(query, category, max_docs=50)
            
            if not filtered_docs:
                return self._fallback_search(query, limit, category)
            
            # 2ë‹¨ê³„: GPTë¥¼ í†µí•œ ìµœì¢… ë¬¸ì„œ ì„ íƒ
            documents_info = []
            for i, doc in enumerate(filtered_docs):
                # ê°„ë‹¨í•œ ë¬¸ì„œ ì •ë³´ë§Œ ìƒì„±
                summary = f"ë¬¸ì„œ {i+1}: {doc.title}\n"
                summary += f"íŒŒì¼: {doc.file_path}\n"
                
                # ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶”ê°€
                if doc.metadata.get('class_name'):
                    summary += f"í´ë˜ìŠ¤: {doc.metadata['class_name']}\n"
                if doc.metadata.get('function_name'):
                    summary += f"í•¨ìˆ˜: {doc.metadata['function_name']}\n"
                if doc.metadata.get('api_endpoint'):
                    summary += f"API: {doc.metadata['api_endpoint']}\n"
                if doc.metadata.get('table_name'):
                    summary += f"í…Œì´ë¸”: {doc.metadata['table_name']}\n"
                
                # ë‚´ìš© ìš”ì•½ (ì²˜ìŒ 100ì)
                content_preview = doc.metadata.get('original_content', doc.content)[:100]
                summary += f"ë‚´ìš©: {content_preview}...\n"
                summary += "---\n"
                
                documents_info.append((i, summary))
            
            # GPTì—ê²Œ ê´€ë ¨ ë¬¸ì„œ ì„ íƒ ìš”ì²­
            prompt = f"""ë‹¤ìŒì€ sabangnet_API ì½”ë“œë² ì´ìŠ¤ì˜ ë¬¸ì„œë“¤ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸: "{query}"

ë‹¤ìŒ ë¬¸ì„œë“¤ ì¤‘ì—ì„œ ì§ˆë¬¸ì— ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œë“¤ì˜ ë²ˆí˜¸ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”. 
ìµœëŒ€ {limit}ê°œì˜ ë¬¸ì„œ ë²ˆí˜¸ë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ëª©ë¡:
{''.join([f"{i+1}. {info}" for i, info in enumerate(documents_info)])}

ê´€ë ¨ ë¬¸ì„œ ë²ˆí˜¸ë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”:"""

            response = self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì½”ë“œë² ì´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì— ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œë“¤ì„ ì„ íƒí•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=100,
                temperature=0.1
            )
            
            # GPT ì‘ë‹µì—ì„œ ë¬¸ì„œ ë²ˆí˜¸ ì¶”ì¶œ
            selected_text = response.choices[0].message.content.strip()
            selected_indices = []
            
            # ìˆ«ì ì¶”ì¶œ
            import re
            numbers = re.findall(r'\d+', selected_text)
            for num_str in numbers:
                try:
                    idx = int(num_str) - 1  # 1-based to 0-based
                    if 0 <= idx < len(filtered_docs):
                        selected_indices.append(idx)
                except ValueError:
                    continue
            
            # ì„ íƒëœ ë¬¸ì„œë“¤ë¡œ ê²°ê³¼ ìƒì„±
            results = []
            for idx in selected_indices[:limit]:
                if idx < len(filtered_docs):
                    doc = filtered_docs[idx]
                    results.append(QueryResult(
                        document=doc,
                        score=1.0 - (len(results) * 0.1),  # ìˆœì„œì— ë”°ë¥¸ ì ìˆ˜
                        matched_sections=[doc.metadata.get('original_content', doc.content)]
                    ))
            
            return results
            
        except Exception as e:
            print(f"GPT ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return self._fallback_search(query, limit, category)
    
    def _pre_filter_documents(self, query: str, category: str = None, max_docs: int = 50) -> List[Document]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ì‚¬ì „ í•„í„°ë§"""
        query_words = re.findall(r'\b\w+\b', query.lower())
        scored_docs = []
        
        for doc in self.documents:
            if category and doc.category != category:
                continue
            
            score = 0
            
            # íŒŒì¼ëª… ë§¤ì¹­ (ë†’ì€ ì ìˆ˜)
            file_name = doc.file_path.lower()
            for word in query_words:
                if word in file_name:
                    score += 3
            
            # ì œëª© ë§¤ì¹­
            title = doc.title.lower()
            for word in query_words:
                if word in title:
                    score += 2
            
            # ë©”íƒ€ë°ì´í„° ë§¤ì¹­
            if doc.metadata.get('class_name'):
                class_name = doc.metadata['class_name'].lower()
                for word in query_words:
                    if word in class_name:
                        score += 2
            
            if doc.metadata.get('function_name'):
                func_name = doc.metadata['function_name'].lower()
                for word in query_words:
                    if word in func_name:
                        score += 2
            
            if doc.metadata.get('api_endpoint'):
                api_endpoint = doc.metadata['api_endpoint'].lower()
                for word in query_words:
                    if word in api_endpoint:
                        score += 2
            
            if doc.metadata.get('table_name'):
                table_name = doc.metadata['table_name'].lower()
                for word in query_words:
                    if word in table_name:
                        score += 2
            
            # ë‚´ìš© ë§¤ì¹­ (ë‚®ì€ ì ìˆ˜)
            content = doc.metadata.get('original_content', doc.content).lower()
            for word in query_words:
                if word in content:
                    score += 1
            
            if score > 0:
                scored_docs.append((score, doc))
        
        # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ ë¬¸ì„œ ë°˜í™˜
        scored_docs.sort(key=lambda x: x[0], reverse=True)
        return [doc for score, doc in scored_docs[:max_docs]]
    
    def _fallback_search(self, query: str, limit: int, category: str = None) -> List[QueryResult]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± ê²€ìƒ‰"""
        query_words = re.findall(r'\b\w+\b', query.lower())
        results = []
        
        for doc in self.documents:
            if category and doc.category != category:
                continue
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            content_lower = doc.content.lower()
            score = sum(1 for word in query_words if word in content_lower)
            score = score / len(query_words) if query_words else 0
            
            if score > 0:
                results.append(QueryResult(
                    document=doc,
                    score=score,
                    matched_sections=[doc.content[:300] + "..." if len(doc.content) > 300 else doc.content]
                ))
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        results.sort(key=lambda x: x.score, reverse=True)
        return results[:limit]
    
    def answer_question(self, question: str, context_limit: int = 5) -> str:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
        # ì§ˆë¬¸ ë¶„ì„
        question_analysis = self._analyze_question_type(question)
        
        # ì „ëµì  ê²€ìƒ‰
        results = self._strategic_search(question, question_analysis, context_limit)
        
        if not results:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        for result in results:
            # ì›ë³¸ ë‚´ìš© ì‚¬ìš© (ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ)
            original_content = result.document.metadata.get('original_content', result.document.content)
            
            context_parts.append(f"íŒŒì¼: {result.document.file_path}")
            context_parts.append(f"ì¹´í…Œê³ ë¦¬: {result.document.category}")
            
            # ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶”ê°€
            if result.document.metadata.get('class_name'):
                context_parts.append(f"í´ë˜ìŠ¤: {result.document.metadata['class_name']}")
            if result.document.metadata.get('function_name'):
                context_parts.append(f"í•¨ìˆ˜: {result.document.metadata['function_name']}")
            if result.document.metadata.get('api_endpoint'):
                context_parts.append(f"API: {result.document.metadata['api_endpoint']}")
            if result.document.metadata.get('table_name'):
                context_parts.append(f"í…Œì´ë¸”: {result.document.metadata['table_name']}")
            
            context_parts.append(f"ë‚´ìš©: {original_content}")
            context_parts.append("---")
        
        context = "\n".join(context_parts)
        
        # GPTë¥¼ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„±
        if self.openai_client:
            try:
                # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¡°ì •
                system_prompt = self._get_system_prompt(question_analysis)
                
                response = self.openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {
                            "role": "system", 
                            "content": system_prompt
                        },
                        {
                            "role": "user",
                            "content": f"""ì§ˆë¬¸: {question}

ê´€ë ¨ ì½”ë“œë² ì´ìŠ¤ ì •ë³´:
{context}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ìƒì„¸í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”."""
                        }
                    ],
                    max_tokens=1000,
                    temperature=0.3
                )
                
                return response.choices[0].message.content.strip()
                
            except Exception as e:
                print(f"GPT API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                # GPT ì‹¤íŒ¨ ì‹œ í´ë°± ë‹µë³€
                return self._generate_fallback_answer(question, context)
        else:
            # GPTê°€ ì—†ì„ ë•Œ í´ë°± ë‹µë³€
            return self._generate_fallback_answer(question, context)
    
    def _analyze_question_type(self, question: str) -> Dict:
        """ì§ˆë¬¸ ìœ í˜•ì„ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ ì „ëµ ê²°ì •"""
        question_lower = question.lower()
        
        analysis = {
            'type': 'general',
            'entities': [],
            'intent': 'unknown',
            'search_strategy': 'semantic'
        }
        
        # 1. ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜
        if any(keyword in question_lower for keyword in ['ì–´ë–»ê²Œ', 'how', 'ë°©ë²•', 'ì‘ë™']):
            analysis['type'] = 'how_to'
            analysis['intent'] = 'process_explanation'
        
        elif any(keyword in question_lower for keyword in ['ì–´ë””ì—', 'where', 'ìœ„ì¹˜', 'íŒŒì¼']):
            analysis['type'] = 'location'
            analysis['intent'] = 'file_location'
        
        elif any(keyword in question_lower for keyword in ['ë¬´ì—‡', 'what', 'êµ¬ì¡°', 'ì„¤ëª…']):
            analysis['type'] = 'what_is'
            analysis['intent'] = 'definition'
        
        # 2. ì—”í‹°í‹° ì¶”ì¶œ
        entities = []
        if 'í…Œì´ë¸”' in question_lower or 'table' in question_lower:
            entities.append('database_table')
        
        if 'api' in question_lower or 'ì—”ë“œí¬ì¸íŠ¸' in question_lower:
            entities.append('api_endpoint')
        
        if 'ì»¨íŠ¸ë¡¤ëŸ¬' in question_lower or 'controller' in question_lower:
            entities.append('controller')
        
        if 'ì„œë¹„ìŠ¤' in question_lower or 'service' in question_lower:
            entities.append('service')
        
        if 'ëª¨ë¸' in question_lower or 'model' in question_lower:
            entities.append('database_model')
        
        analysis['entities'] = entities
        
        # 3. ê²€ìƒ‰ ì „ëµ ê²°ì •
        if analysis['type'] == 'location':
            analysis['search_strategy'] = 'metadata_based'
        elif analysis['type'] == 'how_to':
            analysis['search_strategy'] = 'process_based'
        else:
            analysis['search_strategy'] = 'semantic'
        
        return analysis
    
    def _strategic_search(self, question: str, question_analysis: Dict, limit: int) -> List[QueryResult]:
        """ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ëµì  ê²€ìƒ‰"""
        strategy = question_analysis['search_strategy']
        entities = question_analysis['entities']
        
        if strategy == 'metadata_based':
            # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰ (ìœ„ì¹˜ ì°¾ê¸°)
            return self._metadata_based_search(question, entities, limit)
        elif strategy == 'process_based':
            # í”„ë¡œì„¸ìŠ¤ ê¸°ë°˜ ê²€ìƒ‰ (ë°©ë²• ì„¤ëª…)
            return self._process_based_search(question, entities, limit)
        else:
            # ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ (ì¼ë°˜)
            return self.search(question, limit)
    
    def _metadata_based_search(self, question: str, entities: List[str], limit: int) -> List[QueryResult]:
        """ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰"""
        results = []
        
        for doc in self.documents:
            score = 0
            
            # ì—”í‹°í‹° ë§¤ì¹­
            for entity in entities:
                if entity == 'database_table' and doc.metadata.get('table_name'):
                    score += 2
                elif entity == 'api_endpoint' and doc.metadata.get('api_endpoint'):
                    score += 2
                elif entity == 'controller' and 'controller' in doc.category:
                    score += 2
                elif entity == 'service' and 'service' in doc.category:
                    score += 2
                elif entity == 'database_model' and 'database_model' in doc.category:
                    score += 2
            
            # íŒŒì¼ëª… ë§¤ì¹­
            if any(keyword in doc.file_path.lower() for keyword in question.lower().split()):
                score += 1
            
            if score > 0:
                results.append(QueryResult(
                    document=doc,
                    score=score,
                    matched_sections=[doc.metadata.get('original_content', doc.content)]
                ))
        
        results.sort(key=lambda x: x.score, reverse=True)
        return results[:limit]
    
    def _process_based_search(self, question: str, entities: List[str], limit: int) -> List[QueryResult]:
        """í”„ë¡œì„¸ìŠ¤ ê¸°ë°˜ ê²€ìƒ‰"""
        # í•¨ìˆ˜, API ì—”ë“œí¬ì¸íŠ¸, ì„œë¹„ìŠ¤ ë¡œì§ ì¤‘ì‹¬ìœ¼ë¡œ ê²€ìƒ‰
        results = []
        
        for doc in self.documents:
            score = 0
            
            # í”„ë¡œì„¸ìŠ¤ ê´€ë ¨ ì²­í¬ íƒ€ì… ìš°ì„ 
            if doc.metadata.get('chunk_type') in ['function', 'api_endpoint', 'service']:
                score += 3
            
            # ì—”í‹°í‹° ë§¤ì¹­
            for entity in entities:
                if entity in doc.category:
                    score += 2
            
            # í‚¤ì›Œë“œ ë§¤ì¹­
            question_words = question.lower().split()
            content_lower = doc.content.lower()
            score += sum(1 for word in question_words if word in content_lower)
            
            if score > 0:
                results.append(QueryResult(
                    document=doc,
                    score=score,
                    matched_sections=[doc.metadata.get('original_content', doc.content)]
                ))
        
        results.sort(key=lambda x: x.score, reverse=True)
        return results[:limit]
    
    def _get_system_prompt(self, question_analysis: Dict) -> str:
        """ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        base_prompt = "ë‹¹ì‹ ì€ sabangnet_API ì „ììƒê±°ë˜ ì‹œìŠ¤í…œì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì½”ë“œì™€ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ë‹¨ê³„ë³„ ì„¤ëª…ì„ í¬í•¨í•´ì£¼ì„¸ìš”."
        
        if question_analysis['type'] == 'how_to':
            return base_prompt + "\n\níŠ¹íˆ í”„ë¡œì„¸ìŠ¤ë‚˜ ë°©ë²•ì— ëŒ€í•œ ì§ˆë¬¸ì´ë¯€ë¡œ ë‹¨ê³„ë³„ë¡œ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        elif question_analysis['type'] == 'location':
            return base_prompt + "\n\níŠ¹íˆ íŒŒì¼ ìœ„ì¹˜ë‚˜ êµ¬ì¡°ì— ëŒ€í•œ ì§ˆë¬¸ì´ë¯€ë¡œ ì •í™•í•œ ê²½ë¡œì™€ ìœ„ì¹˜ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”."
        elif question_analysis['type'] == 'what_is':
            return base_prompt + "\n\níŠ¹íˆ ì •ì˜ë‚˜ êµ¬ì¡°ì— ëŒ€í•œ ì§ˆë¬¸ì´ë¯€ë¡œ ëª…í™•í•˜ê³  ìƒì„¸í•œ ì„¤ëª…ì„ ì œê³µí•´ì£¼ì„¸ìš”."
        else:
            return base_prompt
    
    def _generate_fallback_answer(self, question: str, context: str) -> str:
        """GPT ì—†ì„ ë•Œ í´ë°± ë‹µë³€ ìƒì„±"""
        return f"""
ì§ˆë¬¸: {question}

ê´€ë ¨ ì •ë³´:
{context}

ì°¸ê³ : ìœ„ ì •ë³´ëŠ” sabangnet_API ì½”ë“œë² ì´ìŠ¤ì—ì„œ ì¶”ì¶œëœ ë‚´ìš©ì…ë‹ˆë‹¤.
ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•˜ë©´ í•´ë‹¹ íŒŒì¼ì„ ì§ì ‘ í™•ì¸í•´ì£¼ì„¸ìš”.
        """.strip()
    
    def get_statistics(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ í†µê³„ ì •ë³´"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ì´ ë¬¸ì„œ ìˆ˜
        cursor.execute('SELECT COUNT(*) FROM documents')
        total_docs = cursor.fetchone()[0]
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ìˆ˜
        cursor.execute('SELECT category, COUNT(*) FROM documents GROUP BY category')
        category_stats = dict(cursor.fetchall())
        
        conn.close()
        
        return {
            "total_documents": total_docs,
            "category_statistics": category_stats,
            "vector_available": False,
            "faiss_index_built": False,
            "gpt_available": self.openai_client is not None,
            "search_mode": "GPT ê¸°ë°˜ ê²€ìƒ‰"
        }

# ì‹¤ì‹œê°„ ì§ˆë¬¸-ë‹µë³€ ì‹œìŠ¤í…œ
class InteractiveQASystem:
    """ì‹¤ì‹œê°„ ì§ˆë¬¸-ë‹µë³€ ì‹œìŠ¤í…œ"""
    
    def __init__(self, rag_system: SabangnetRAGSystem):
        self.rag = rag_system
    
    def start_interactive_mode(self):
        """ëŒ€í™”í˜• ëª¨ë“œ ì‹œì‘"""
        print("ğŸš€ sabangnet_API RAG ì‹œìŠ¤í…œ ì‹œì‘")
        print("=" * 60)
        print("ğŸ’¡ ì‚¬ìš©ë²•:")
        print("  - sabangnet_APIì— ëŒ€í•œ ì–´ë–¤ ì§ˆë¬¸ì´ë“  ììœ ë¡­ê²Œ ì…ë ¥í•˜ì„¸ìš”")
        print("  - ì˜ˆ: 'ì£¼ë¬¸ ë°ì´í„°ëŠ” ì–´ë–»ê²Œ ì²˜ë¦¬ë˜ë‚˜ìš”?', 'ìƒí’ˆ ë“±ë¡ APIëŠ” ì–´ë””ì— ìˆë‚˜ìš”?'")
        print("  - ì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
        print("=" * 60)
        
        while True:
            try:
                question = input("\nâ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                
                if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                    print("ğŸ‘‹ RAG ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                
                if not question:
                    continue
                
                print("\nğŸ” ê²€ìƒ‰ ì¤‘...")
                answer = self.rag.answer_question(question)
                print(f"\nğŸ’¡ ë‹µë³€:\n{answer}")
                print("-" * 60)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ RAG ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def demo_questions(self):
        """ë°ëª¨ ì§ˆë¬¸ë“¤ë¡œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        demo_questions = [
            "ë°ì´í„°ë² ì´ìŠ¤ëŠ” ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆë‚˜ìš”?",
            "ì£¼ë¬¸ ê´€ë ¨ ì»¨íŠ¸ë¡¤ëŸ¬ëŠ” ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?",
            "ì§€ë§ˆì¼“ì— ë¬¼ê±´ì„ ë³´ë‚´ë ¤ë©´ ì–´ë–¤ APIë¥¼ ì¨ì•¼ í•˜ë‚˜ìš”?",
            "ë°°ì¹˜ ì‘ì—…ì€ ì–´ë–»ê²Œ êµ¬í˜„ë˜ì–´ ìˆë‚˜ìš”?",
            "ìŠ¤ë§ˆì¼ë°°ì†¡ ì£¼ë¬¸ ë°ì´í„°ëŠ” ì–´ë–»ê²Œ ì²˜ë¦¬ë˜ë‚˜ìš”?"
        ]
        
        print("ğŸ§ª ë°ëª¨ ì§ˆë¬¸ìœ¼ë¡œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
        print("=" * 60)
        
        for i, question in enumerate(demo_questions, 1):
            print(f"\n[{i}/{len(demo_questions)}] ì§ˆë¬¸: {question}")
            print("-" * 40)
            
            answer = self.rag.answer_question(question)
            print(f"ë‹µë³€: {answer}")
            print("=" * 60)

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag = SabangnetRAGSystem()
    
    # sabangnet_APIì—ì„œ ë¬¸ì„œ ì¶”ì¶œ
    rag.extract_sabangnet_documents()
    
    # Faiss ì¸ë±ìŠ¤ ì¬êµ¬ì¶•
    rag._build_faiss_index()
    
    # í†µê³„ ì •ë³´ ì¶œë ¥
    stats = rag.get_statistics()
    print(f"ì‹œìŠ¤í…œ í†µê³„: {stats}")

    # ì‹¤ì‹œê°„ ì§ˆë¬¸-ë‹µë³€ ì‹œìŠ¤í…œ ì‹œì‘
    qa_system = InteractiveQASystem(rag)
    qa_system.start_interactive_mode()
